{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import Dataset\n",
    "dat = pd.read_csv('fin_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at albert-base-v2 were not used when initializing TFAlbertModel: ['predictions']\n",
      "- This IS expected if you are initializing TFAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFAlbertModel were initialized from the model checkpoint at albert-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#Importing of Packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AlbertTokenizer, TFAlbertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,Dense,Bidirectional,Conv2D,MaxPooling2D,Flatten,concatenate,GlobalAveragePooling2D,BatchNormalization,Lambda,Add,Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFBertModel, logging\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "bert = TFAlbertModel.from_pretrained('albert-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparation Of Data Pipelines\n",
    "#https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "\n",
    "\n",
    "class preparation():\n",
    "    def __init__(self,df,tokenizer,bert,max_len_tweet,max_len_text_data,tweet=True,tweet_text=True,image=True):\n",
    "        self.df=df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bert = bert\n",
    "        self.max_len_tweet=max_len_tweet\n",
    "        self.max_len_text_data=max_len_text_data\n",
    "        self.tweet=tweet\n",
    "        self.tweet_text=tweet_text\n",
    "        self.image=image\n",
    "        self.tokenizer=tokenizer\n",
    "        self.bert=bert\n",
    "        \n",
    "        \n",
    "        \n",
    "    def tokenize(self,sentences,max_len):\n",
    "        \n",
    "        input_ids=[]\n",
    "        attention_masks=[]\n",
    "        for sent in sentences:\n",
    "            bert_inp=self.tokenizer.encode_plus(sent,max_length=max_len,add_special_tokens = True,pad_to_max_length = True,return_attention_mask = True,truncation = True)\n",
    "            input_ids.append(bert_inp['input_ids'])\n",
    "            attention_masks.append(bert_inp['attention_mask'])\n",
    "\n",
    "        input_ids=np.asarray(input_ids)\n",
    "        attention_masks=np.array(attention_masks)\n",
    "        return input_ids,attention_masks\n",
    "    \n",
    "    \n",
    "    def prep_txt(self,input_ids, masks, input_ids2, masks2, path, labels):\n",
    "        image = tf.io.read_file(path)\n",
    "        image = tf.io.decode_image(image, channels=3,expand_animations = False)\n",
    "        image = tf.cast(image, tf.float32) / 255.0 \n",
    "        image = tf.image.resize(image, size=(200, 200)) \n",
    "        return {'input_ids': input_ids, 'attention_mask': masks, \n",
    "            'input_ids2': input_ids2, 'attention_mask2': masks2,'images': image}, labels\n",
    "\n",
    "\n",
    "    def prep_image(self,path, labels):\n",
    "        image = tf.io.read_file(path)\n",
    "        image = tf.io.decode_image(image, channels=3,expand_animations = False)\n",
    "        image = tf.cast(image, tf.float32) / 255.0 \n",
    "        image = tf.image.resize(image, size=(200, 200)) \n",
    "        return {'images1': image}, labels\n",
    "\n",
    "    def prep_tweet(self,input_ids, masks, labels):\n",
    "        return {'input_ids': input_ids, 'attention_mask': masks}, labels\n",
    "    \n",
    "    def prep_txt_imtxt_image(self,input_ids, masks, input_ids2, masks2,labels):\n",
    "        return {'input_ids': input_ids, 'attention_mask': masks, \n",
    "            'input_ids2': input_ids2, 'attention_mask2': masks2}, labels\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def prep_data_pipeline(self):\n",
    "        sentences=self.df['tweet']\n",
    "        sentences1=self.df['text_data']\n",
    "        input_ids,attention_masks=self.tokenize(sentences,self.max_len_tweet)\n",
    "        input_ids1,attention_masks1=self.tokenize(sentences1,self.max_len_text_data)\n",
    "        paths =[img for i,img in enumerate(self.df['Path'])]\n",
    "        labels=self.df['Target'].to_numpy()#.reshape((-1,1))\n",
    "        if self.tweet:\n",
    "            if self.tweet_text:\n",
    "                if self.image:\n",
    "                    dataset = tf.data.Dataset.from_tensor_slices((input_ids, attention_masks, input_ids1, attention_masks1,paths,labels)).map(self.prep_txt).batch(5)\n",
    "                    \n",
    "                else:\n",
    "                    dataset = tf.data.Dataset.from_tensor_slices((input_ids, attention_masks, input_ids1, attention_masks1,labels)).map(self.prep_txt_imtxt_image).batch(5)\n",
    "                    \n",
    "            \n",
    "            elif self.tweet_text==False:\n",
    "                if self.image==False:\n",
    "                    dataset = tf.data.Dataset.from_tensor_slices((input_ids, attention_masks,labels)).map(self.prep_tweet).batch(5)\n",
    "                    \n",
    "                \n",
    "                \n",
    "        elif self.tweet==False:\n",
    "            if self.tweet_text==False:\n",
    "                if self.image:\n",
    "                    dataset=tf.data.Dataset.from_tensor_slices((paths,labels)).map(self.prep_image).batch(5)\n",
    "                        \n",
    "                    \n",
    "        return dataset     \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lohith\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_1,test_1=train_test_split(dat, test_size=.25, random_state=42)\n",
    "train=preparation(df=train_1,tokenizer=tokenizer,bert=bert,max_len_tweet=250,max_len_text_data=245,tweet=True,tweet_text=True,image=True).prep_data_pipeline()\n",
    "test=preparation(df=test_1,tokenizer=tokenizer,bert=bert,max_len_tweet=250,max_len_text_data=245,tweet=True,tweet_text=True,image=True).prep_data_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "       \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        query=inputs\n",
    "        key,val=tf.transpose(inputs),tf.transpose(inputs)\n",
    "        dk = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  \n",
    "        scaled_attention_logits =tf.matmul(query,key)/tf.math.sqrt(dk)\n",
    "        attention_weights =tf.keras.activations.softmax(scaled_attention_logits ,axis=1)\n",
    "        \n",
    "        out=tf.transpose(tf.matmul(val,attention_weights))\n",
    "       \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.BatchNormalization = tf.keras.layers.BatchNormalization()\n",
    "        self.Dense1 = tf.keras.layers.Dense(115, activation='relu', kernel_regularizer=l2(0.01))\n",
    "        self.Dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.Dense2 = tf.keras.layers.Dense(1, activation='sigmoid', name='outputs')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.BatchNormalization(inputs)\n",
    "        x = self.Dense1(x)\n",
    "        x = self.Dropout(x)\n",
    "        x = self.Dense2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {
    "GMU.JPG": {
     "image/jpeg": "/9j/4AAQSkZJRgABAQEAYABgAAD/4RD4RXhpZgAATU0AKgAAAAgABAE7AAIAAAAPAAAISodpAAQAAAABAAAIWpydAAEAAAAeAAAQ0uocAAcAAAgMAAAAPgAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAExvaGl0aCBNYWNoYW5pAAAABZADAAIAAAAUAAAQqJAEAAIAAAAUAAAQvJKRAAIAAAADMTIAAJKSAAIAAAADMTIAAOocAAcAAAgMAAAInAAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADIwMjM6MDM6MDIgMTA6NTE6MzkAMjAyMzowMzowMiAxMDo1MTozOQAAAEwAbwBoAGkAdABoACAATQBhAGMAaABhAG4AaQAAAP/hCyFodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvADw/eHBhY2tldCBiZWdpbj0n77u/JyBpZD0nVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkJz8+DQo8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIj48cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPjxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSJ1dWlkOmZhZjViZGQ1LWJhM2QtMTFkYS1hZDMxLWQzM2Q3NTE4MmYxYiIgeG1sbnM6ZGM9Imh0dHA6Ly9wdXJsLm9yZy9kYy9lbGVtZW50cy8xLjEvIi8+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iPjx4bXA6Q3JlYXRlRGF0ZT4yMDIzLTAzLTAyVDEwOjUxOjM5LjEyMjwveG1wOkNyZWF0ZURhdGU+PC9yZGY6RGVzY3JpcHRpb24+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczpkYz0iaHR0cDovL3B1cmwub3JnL2RjL2VsZW1lbnRzLzEuMS8iPjxkYzpjcmVhdG9yPjxyZGY6U2VxIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+PHJkZjpsaT5Mb2hpdGggTWFjaGFuaTwvcmRmOmxpPjwvcmRmOlNlcT4NCgkJCTwvZGM6Y3JlYXRvcj48L3JkZjpEZXNjcmlwdGlvbj48L3JkZjpSREY+PC94OnhtcG1ldGE+DQogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgIDw/eHBhY2tldCBlbmQ9J3cnPz7/2wBDAAcFBQYFBAcGBQYIBwcIChELCgkJChUPEAwRGBUaGRgVGBcbHichGx0lHRcYIi4iJSgpKywrGiAvMy8qMicqKyr/2wBDAQcICAoJChQLCxQqHBgcKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKir/wAARCAE2AY0DASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD6RooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArBGvLF4yu9Mu7y1SFLaB4oyNshkkd1xnPI+VccDlgK3q4KPQ9b+w6veT2IkvdS15bhoDMuVtYHVYQGzjDCFWIzx5rcZ4oA72imReZ5KeftMm0b9g4z3xntT6ACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiq1/qVnpdqbnUbmO3hBxvkbHPoPU+1YH/AAsTw5/z9yn3FvJ/hUuUVuxXSOoorl/+Fi+HP+fqX/wGk/wo/wCFi+HP+fqX/wABpP8AClzx7hzI6iiuX/4WL4c/5+pf/AaT/Cj/AIWL4c/5+pf/AAGk/wAKOePcOZHUUVy//CxfDn/P1L/4DSf4Uf8ACxfDn/P1L/4DSf4Uc8e4cyOoorl/+Fi+HP8An6l/8BpP8KP+Fi+HP+fqX/wGk/wo549w5kdRXPat4vs9I8a6D4dnx5+srOYz/dMahsfjk/lUH/CxfDn/AD9S/wDgNJ/hXzT8R/iLqup/GzTtb0rTbtYdLlVLCGSFla5WNiWYAjOGyfopGaalF7MLo+vqK5Cx+Jvh28sILlpLmBpY1cxS2sgaMkZ2njqOlT/8LF8Of8/Uv/gNJ/hS549w5kdRRXL/APCxfDn/AD9S/wDgNJ/hR/wsXw5/z9S/+A0n+FHPHuHMjqKK5f8A4WL4c/5+pf8AwGk/wo/4WL4c/wCfqX/wGk/wo549w5kdRRXL/wDCxfDn/P1L/wCA0n+FH/CxfDn/AD9S/wDgNJ/hRzx7hzI6iiuX/wCFi+HP+fqX/wABpP8ACj/hYvhz/n6l/wDAaT/Cjnj3DmR1FFcv/wALF8Of8/Uv/gNJ/hR/wsXw5/z9S/8AgNJ/hRzx7hzI6iiuX/4WJ4c/5+5f/AeT/Ct7T9TstVthcadcxXER/ijbOPY+h9jTUovZhdMtUUUVQwooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAOHvol1T4nPFeASw6dZq8MTDKh2PLY9f8BXRVgJ/yVLVP+vGL+db9cE/iZg9woooqRBRRRQAUUUUAFFNSWORpFjdWaNtrgHJU4BwfQ4IP41neI9ch8OeHrvVblGkWBRsiT70rsQqIPdmKj8aAINe8T22htFbx21xqWoTf6qwslVpWH987iFVBg/MxA7DJ4rkNb0DxDrvjvw74nFnp9t/YomxaPfMzTeYuOWEWFx/wKug8O6GNItZprhmm1O/cT39wzlvMlwAQM9EXoqjgD8a02uoUvI7RpAJ5UaREx1VSoY/huX866IwSPSp4WKXvblOx8XRvq/8AZWtWU2k3j/6gysHhufaOUcE/7LBW9jXRVialp1rq+mz2F/H5lvcJsdQxU49QRyCDyCOQRms/wvfSafrV34Wv724vJ4IlvLSe5ILywMdrAt3KyA9uFZKzlC2qOetQ9muZbHV0UE4602KVJoklhdXjdQyupyGB6EGszlHUUUUAFFFFABRRRQAUUUUAFc7aqukfEi2hsFEUOqW7tcRr91nTJDY7H/E10Vc9c/8AJT9C/wCvaf8AkaqHxIa3O3ooorvNwooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAOKT/kqWqf9eMX8636wE/5Klqn/XjF/Ot+uCfxMwe4Vx/xB8cS+ErSytNJsv7S13VJvJsLPsxyNzMcjgZA6jJI6DJHYV5X4/8A+S6/Dn/eu/8A0AURV2C3LkOifFm7hS4ufFmj2Esg3Naxaf5ixH+6GJ5p/wDwjnxT/wCh50v/AMFQ/wAa9Foo5guedf8ACOfFP/oedL/8FQ/xo/4Rz4p/9Dzpf/gqH+Nei0UczC58x6t4F+Kd58VdQn027nkut8Yl1e3zZwMREmOe4AIGBuyQcivV9dtdb0vwLYf8JlqlrqjQ6nYGaWC2MOP9JjALNuwcMQc7RnHSvRaq6np1rrGlXOnahEJrW6iaKVD/ABKRg1XPdq5SlZpkBzg461ztyl9/wmmmg3Ntu+w3WP8ARm/vwdvMp+i6rPaXKaB4h22+px7ltnZ+NQiQD96nvgjcvVT7EE7xVd4YgbgMA45Fbp9T2FJTV0JGHEYErKz9yq7Qfwyf51zN4JpvitpaWEkcdxHot4Xd4zIqBpYAm5QR1KtjnsfStPW9ft9IVIEU3ep3CsbPT4mHm3DDsB2HTLHgdzUnhPSdQtludV8RxWqa1fFVmW1YtHFGmQiKx5I5Zz7uaiTsjnxNRKHL1PGfin4G+K2o3Mkram+uae2AILAGJUHvb55+uXP0rsfDnhv4njwtpQt/GOn2sQs4dlvLpeXiXYMI2e46H6V6vRWXO7WPO5jzr/hHPin/ANDzpf8A4Kh/jR/wjnxT/wCh50v/AMFQ/wAa9FopczFc86/4Rz4p/wDQ86X/AOCof40f8I58U/8AoedL/wDBUP8AGvRaKOZhc86/4Rz4p/8AQ86X/wCCof40yXQ/izbQvNb+LtHvZYxuW3k07YspH8JYdM+tekUUczC5yHw+8aXHiuzvrfWbJdN1vTJ/JvbIE5TIyrYPY89yODgkV19eZeEuP2gvHmP+faz/APQBXptEtwYVz1z/AMlP0L/r2n/ka6Gueuf+Sn6F/wBe0/8AI0R+JAtzt6KKK7zcKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDik/5Klqn/AF4xfzrfrAT/AJKlqn/XjF/Ot+uCfxMwe4V5X4//AOS7fDn63f8A6AK9Uryvx/8A8l2+HP1u/wD0AU4bgtz1SiiioEFFFFABRRRQBS1XRtO1y0Ftq1nFdxK4kRZFzscdGU9VI9Rg1nHwhZbhsv8AWI0/uLqcxH6sT+tb1FNNrYpSktmZumeHtL0i4luLG2xczALLcyyNLK4HQGRyWIGTxnHNaVFFLcm99wooqK6u7extnuL24it4EGXlmcIq/UngUAS0VyaeOJNQuXj8P+HtS1CBTj7bKFtoGP8AsmQhnH+0qkH1ouPGd9psgOseF9Rjts/Nc2TJdrGPVlUiT8kNVySNPZTtex1lFVrDUbPVLNbrTrmK5gbo8bZGfT2PtVmpMwooooA8y8J/8nB+O/8Ar1s//QBXpteZeE/+Tg/Hf/XrZ/8AoAr02qluNhXPXP8AyU/Qv+vaf+Rroa565/5KfoX/AF7T/wAjRH4kC3O3ooorvNwooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAOKT/kqWqf8AXjF/Ot+sBP8AkqWqf9eMX86364J/EzB7hXjXxC8Q6NF8cfBMkuq2aJpjXIvmM6gWxZBtEhz8ufevSfGDS/8ACOPbwTyW7Xs8NmZoW2vGssiozKezbScHscVxH9oTeDZYvCcEUF204jWwunhASJXcpi5xxkY+U8eYfl65NXTjfU3pUudXbPUY5EljWSJ1dGGVZTkEeoNOrgvDdo/gnVrmxe9vNQs7m0N75KW5cxTLIFk8uNAdqtvB2Dgbfc1wXjX9o240y/udO8P6DJDLCSjTakhV1PY+TwR2PzH8Kn2bvZGcqbjLlPeqK8usvjZbfYLfz/CniqWTyl3yLpow5xyR83epv+F12X/Qn+LP/Bb/APZUuSRFmel0V5p/wuuy/wChP8Wf+C3/AOyo/wCF12X/AEJ/iz/wW/8A2VHJILM9LorzT/hdliOvhDxYB/2Df/sq3tN+KPhDVNButXg1iOO2swv2lZkZJIixIAKEZJJBAxnJBo5ZILM62ivNT8b9HlJfTvD3iTULY/6u6ttOJjkHqpJBxXD+Jf2htT0jxhH9g0O5XTHtUL2WqwGCXfufLqwyQCMDkEfKeKapyYcrPoKuLtm/4THVG1K8hb+yLG4K6bDIBieRCVa4ZccjOQnsN3cYo2HxUt9e+Huq67Y6VqdpLaWM9wguLZvKYohPyyqCh/MV0mi262mg2FsnKw20cYPrhQKuEbPU68LBOTb6F2isbTw8HibVo2uJ5oxBbyBZHLbSTLnaO3QcD0q9/adv/cuv/AOb/wCJrQ9G5i61D/wjN5P4r05zBGu2TWIFQMt1AgIMmOokRTkEfeC7TnjHZwTx3NvHPA4kilQOjr0ZSMgj8KoFVdSrgMrDBBHUV5pZfE618L6HLoUXh3xDcHTGntYbm1sQ8O2OR1TDZ6BQo/Cs5xvqjgxVNJqSPXaR3WONndgqqCWZjgAeprwPwV+0bdahdW1h4i0GW4mlwgm0xCzse58o9R3+U/hXoXia3i8a39lpbS31naravehXieFmlV1VN8bgblXLEqRg5GajkadmcsablLlOc8F+ItFu/j54umtdWspotQt7VbR0nUi4Kp8wQ5+YjHavXq8ik1ifxvey+CZorS0MYk/tC+t2SRHWNgNtuGB+Y5wx58rkdSprvPBMk66FJYXMzXDabdSWSTOctJGhGwsT1YKVBPcgmqqRtqaVaXIrpnQ1z1z/AMlP0L/r2n/ka6Gueuf+Sn6F/wBe0/8AI1EfiRgtzt6KKK7zcKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDik/5Klqn/XjF/Ot+sBP+Spap/wBeMX86n0PX11q51SJYViFhevaK4k3CbaqlmHAwAzFT7qa4ZfEzB7lrW9KTWtGuLB5pLcygFJ4sbonUhkdc8ZVgDz6V55BZ63oVrNoF34Th1qfUWctenUY9l+epeUSYdeOdqq4UYC9K9Rrwv4mfEO30f46+FoFvNtrpJIvdr/Kpm+V93P8ACmG56Zp029kaU6kofCek+CfCE3h2GS61S9lvNQuIkiw8pkW2iUkrCjEAsBuOXblup7Vs634c0fxHZta65pttexMCP30YYrnup6qfcVoqyuoZGDKwyGByCPWlqG23czbbd2R20C2trFbxZ2RIEXJycAYFS5pKKQhc0ZpKKAFzXi/xC8PaXc/H3wQZbKBhfeablfLXEpiBdd3HPJwc9gK9nryzx3/yXv4d/wC7ef8AourhuNbnqagIoVQFUDAA4xWLdeD9Cv8AxKuvX+nQ3WoJAtvHJOu8RorMw2qeAcseetbVFQIiurWK7sprSdQYZo2jdcZG0jB/nXMeEtQF3oUdrLuW903FleROCGSVAAc+zDDA9wwNdZXPazoN4dZi1rQJY0uwvl3VrM5WK9TB27iASrqejAHgkHjGNISs9TooVVTlrsxbfRbS11OW/iNx9olGHL3UjKw5wNpbGBuOBjjPFX65618Z2Bd4dagutCuozhotRiMan3STlHHuG+uOlOu/GmkQxj+z3m1idziO30uI3DMewJX5VHuxAHc1senzwte5o63qg0bRbm/8ppniT91Cn3ppCcJGPdmIUe5qXw5pVxZ+DbPT9WIN00BN35Z482TLSbT6bmbFUtN0a91XUIdX8RRfZ1hCvZ6YJdwgfn95KR8rScgADKrjgk8109YzlfRHnYiqpuy2Rl6D4Z0bwzYx2mhabb2UUaBMxoNzAf3m6sfck1meNvC974isUbSNTfTr2JHjLKOJ4nxviLdUztXDrypGRmtfStUOpfat8SwmC5lgQCTcXCNtLdBjkEY56davswRSzkKoGST2qLtO5zJtO6PLp7DX9a02Lw/p3hKLw9JZspjvo9Qi8qxI5DRCP52JGflKqGGQ2M16B4f0WHw/osVhBI8xVnklmk+9NK7F3c+5ZicdBnA6V5B8OviePEXxy162SQjTtUTFmjN0aFcKwH+2gY/lXuNXNy2ZpUqSn8QVz1z/AMlP0L/r2n/ka6Gueuf+Sn6F/wBe0/8AI1MfiRmtzt6KKK7zcKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDiQdvxQ1U4JxYRHA781iaRot1pd94bSHStm9J77ULjAB+1SBdwc8npJKfqiitxP8AkqWqf9eMX86zJPix4QjmkjN9dsY3aNjHpl065UkHDCMg8g9K4pX5mY63djS0XxNLrN55aWKxwie6haTzixBgcRk42jgvvXr/AA5Gc8eXePvBfh4fGzwbb/2XH5WrS3cl8pdz57bQck5z1PbFdzF8U/BMLyPDNdxtKd0jJo10C59TiLmuE8X+NdG1T4seC9ZsTfS2Glm4+1zf2bcr5e5QF4MeTk+gNEbpjSdz2yxsrfTbCCysoxFbW8YjijBJCKBgDnngVPXF/wDC2/B//P5ff+Ci7/8AjVH/AAtvwh/z+X3/AIKLv/41UWZPK+x2lFVNL1S01rSrfUtNlM1rcoJInKFdyn2YAj6EVbpCCiiigAryzx3/AMl7+Hf+7ef+i69Tryzx3/yXv4d/7t5/6Lq4bjW56nRRRUCCiiigBenSjNJRQAUyaQxQSSKjSFVLBF6tgdBT6rajqFtpOmXOoX8nlWtrG0sr7S21QMk4AJP0FAHIaJpF9pw8MyXGnZvPs80988YCgXUxQvvb0G6U++AOwrW8N3zeKPD15PqKI1veTTxpCsmf3GSi8gA4ZRuB64es3/hbfhD/AJ/L7/wUXf8A8aqOH4qeCreMpbz3cSkliqaNdKMnqeIqt3fQqz7HHeD/AAV4ei+PPiq3i0yNItMitJrMLI4MLlQSQd2eT617TXiXh7xxolh8YfFmuXT3yadqMFsltN/ZlyfMKLhhtEeRj3Aruf8AhbfhD/n8vv8AwUXf/wAaoldsGmdpXPXP/JT9C/69p/5Gs1fi14OaSNDf3aGR1jUyaZdIu5iFAyYwBkkDmtK5/wCSn6F/17T/AMjSimpIVmmdvRRRXebhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAcUn/JUtT/68Yf51U8HXMNn4CtLi6lSGGJZXeSRgqqBI+SSatp/yVLU/wDrxh/nVDwvb/a/hxFAEVzJDOqq3Qku+P1rxsb8Pz/zHR+NnQQaja3Gnx3qTxiCRFcOzgAAgEZPY81M88Uf+sljTjPzOBx61w4tNRY+G7b+xWitbRSZrsRhpLeQW6rxHnB3FpF3EMBt6chqq22l6hLP4H+1aTcD+yHdbqR0UlF+zMgzg8jzCBxkcA9Oa4fZrudNzrNX106ZqWiwrHFJb6ncvbtMZceViGSUMOMEfuyOo61sK6uoZGDKRkFTkGuFbTbq4k8OA6bP5dnrt3dSq8QAjiZbkRtj6yx8DkZ7YNdH4aSWLTp45oJID9tuXRZE2/I8zshH/ASKmUUloHUTwR/yJtj/ANtP/RjVvVg+CP8AkTbH/tp/6Mat6vYPOe4UUUUCCvLPHf8AyXv4d/7t5/6Lr1OvLPHf/Je/h3/u3n/ourhuNbnqdFFFQIKKKKACiiigArE8af8AIjaz/wBecn8q26xPGn/Ijaz/ANecn8qBrcs6rrFlpH2c30yxm6uEt4VLAF3Y8AZ9Bkn2BqyZ4hIiGWMPIMopcZb6DvWD4z0+a+t9Me3tDdfZdVt55UUAsIw/zEA9cA5wOTjjNYGu6bqmoQ6lN/ZLxXckFoIkt41YybCX5dmIXYWbAXDehORjx4wUluejfWx3pniU4aaMEHaQXA59PrWTba88ni3UtHuYooY7S3t545/N/wBZ5pkXaQQMEGI9Cc5rk9f0i81K28XyLoswvNT0aOC3AQMGm2yZUNnAILrycdOvHD9U0i6vNQ8TTyaVPP8AbfD0FnbExA75R5+5OTx/rE5OB78GrUIpO7/rQFqzW+Jv/Ih3P/X1af8ApTFWjc/8lQ0P/r2n/kaxfHfmf8KuHnKyyeZY71cYIP2iLOfxrauf+SoaH/17T/yNduD0Xz/yOatujt6KKK9kQUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHFJ/yVLVP+vGH+dZFh4a8ZaVZrZ6f4j0hbaNmMay6Q7MAWLYJ88Z6+la6f8lS1T/rxi/nW/Xn1EpNpoy5nFuxyX9lePP+hl0T/wAEsn/yRWbe3Hi+w1zTdKuPFWhrdan5v2dDo75by13Nx9o9DXf18y/FH4hzp8cNKvNOtpZbfw/KIkUA5nYNmbb7EHZ/wGphRhJ2silUm+p7b/ZXjz/oZdE/8Esn/wAkUf2V48/6GXRP/BLJ/wDJFdLp99Bqem219aMWguYlljJGDtYZGferFR7KH8qF7SfczfDuly6L4ftNPuJ0uJYVIeVI9iuxYkkLk469MmtKiitDMKKKRgShCnaSODjOKAMqy1q4udWexn0q4tuGdJGZWXYDgFtpOwscEKeSDnsQOC8d/wDJe/h3/u3n/ouu70rw6NK1Ge5TUb2eOXlbeVl2I5A3PkKGdmwOXLY6DA4rhPHf/Je/h3/u3n/outI2voNbnqdFFFZiCiiigAooooAKz9f019Y8O3+mwzLBJdQNEsrJvCEjqVyM/TIrQooA5I6X48J/5GXRP/BLJ/8AJFH9lePP+hl0T/wSyf8AyRXW1DeXUVjYz3dwcRQRtK5H91Rk/wAqz9lD+VGntJ9zi7YeM7vVL3T4vFOhm4sfL85P7GfI3ruU/wDHx0xV3+yvHn/Qy6J/4JZP/kivEvhl4/vrn48X1/rAlhj1rdDKrH5YehhDf7uAgP8AtGvpqrlRhF7IbqTXU4TV/CXjDXtP+wan4l0o2rTRSSCDSHR22SK4AJnOMlQOlbVz/wAlP0P/AK9p/wCRroa565/5KfoX/XtP/I1VNKLSSJcnJq529FFFegahRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAcUn/JUtU/68Yv51v1gJ/yVLVP+vGL+db9cE/iZg9wryvx+f+L7fDn63f8A6AK9Uryvx/8A8l2+HP1u/wD0AU4bgtz1SiiioEFFFFABRQSAMk4HqaKACvLPHf8AyXv4d/7t5/6Lrtry/wBYl8Q3Gn6W1jHHb2sMzNcxO5YyNIMDawwB5f61zeteDNf1nxroXiOXU9Mim0USiOFbWTbJ5i4Of3mePap9tThK0maxpyeqPQaK5q81DxDpi2892+lzQPdwQOsUMqtiSVY8glyON2enaulojKMleLIlFx3CijPOO9FUSFFFFABRRRQAUUUUAeZ+Ez/xkH47/wCvWz/9AFemV5l4T/5OD8d/9etn/wCgCvTaqW42Fc9c/wDJT9C/69p/5Guhrnrn/kp+hf8AXtP/ACNEfiQLc7eiiiu83CiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA4pP+Spap/wBeMX863656RxbfFW7Wb5ftVghiJ/i2nkD36/lXQ1wT+JmD3CvFvjLql3ovxO8Ealpti2oXcAufJtVJBldtiAcAnq3pXtNeU/EAbvjr8Olbkbro4PsoI/UA/hThuEdyzDqHxnuolnTR/C9osnIguZZTIg9G2sRn6Gn/AGj40/8APj4P/wC/k/8AjXpVFLm8guea/aPjT/z4+D/+/k/+NH2j40/8+Pg//v5P/jXpVFHN5Bc8g8UXPxdXwjqx1S08JpZCylNw0ck25Y9h3EZOM4z1rl/hQfjFBHAkMG7SHGV/t0sEReOU583p0A+X6V9DOiyIUkUMrDBVhkEUtVz6WsO+hz2ni4Hi/UftjRNP/Z1nvMIIXO+46A81y8llLr9wJdRutTbU5NU2C0tbieCO0tY5SCGCED5owW3NyxYbTgKB1l1ZavD4iub/AE2CxniuLWGErcXLxMpjaQ/wxtkHzB+VP83xL/0DtJ/8GMv/AMYrz6lOo6jlFHTCcVFJsreIYFtfD9lbwbtsWoWCJ5jlzgXUQGWJJP1JzXPfEV/idHp0i+DI9NliZcM8AIuk9doc7D/P0FdDe2Wv6olvBc2mmQQpd287yR3sjsBHKshAUxAEnbjqOtdHW2HjKnH3l1M6sk2rHzp8LG+K1udbisLaxkulnjF3/wAJA8/ng7Ttxk5Kkev4V6D9o+NP/Pj4P/7+T/416SFUOWCjcQATjkgdP50tdLnd3sY3PNftHxp/58fB/wD38n/xo+0fGn/nx8H/APfyf/GvSqKXN5Bc81+0fGn/AJ8fB/8A38n/AMaPtHxp/wCfHwf/AN/J/wDGvSqKObyC55r9o+NP/Pj4P/7+T/40yW/+NFtE0zaR4WuggyYYJZQ7j0XcwGfqa9Noo5vILni/wi1658S/Fnxlqd/p7abdSW9uk1qzFjGyEoRkgH+HPI4zXtFeXeDgF/aG8ehRgG3tTgeu0V6jTnuD3Cueuf8Akp+hf9e0/wDI10Nc+R9q+KGm+R8/2S0laYrzs3ZAB9DSh8SBbnbUUUV3m4UUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAGfq+h6frlusWpW4lCHdG4JVkPqGHIrI/4Qe37azrQHp9tP8AhXT0VLjF7oVkzmP+EHt/+g1rX/gaf8K8v8d+GYrb43/D6zGo6lItybrMklyS6YQfdOOK93ryP4if8nAfDT63f/oApckV0DlR2v8Awg9v/wBBrWv/AANP+FH/AAg9v/0Gta/8DT/hXT0Uezh2Fyo5j/hB7f8A6DWtf+Bp/wAKP+EHt/8AoNa1/wCBp/wrp6KPZw7Byo5j/hB7f/oNa1/4Gn/Cj/hB7f8A6DWtf+Bp/wAK6eij2cOwcqOY/wCEHt/+g1rX/gaf8KP+EHt/+g1rX/gaf8K6eij2cOwcqOY/4Qe3/wCg1rX/AIGn/Cj/AIQe3/6DWtf+Bp/wrp6KPZw7Byo5j/hB7f8A6DWtf+Bp/wAKP+EHt/8AoNa1/wCBp/wrp6KPZw7Byo5j/hB7f/oNa1/4Gn/Cj/hB7f8A6DWtf+Bp/wAK6eij2cOwcqOY/wCEHt/+g1rX/gaf8KP+EHt/+g1rX/gaf8K6eij2cOwcqOY/4Qe3/wCg1rX/AIGn/Cj/AIQe3/6DWtf+Bp/wrp6KPZw7Byo8M8JeGIp/j944sjqWpIsFvakSpckSPlB95sc16Z/wg9v/ANBrWv8AwNP+Fcd4L/5OQ+IH/XrZ/wDoAr1ijki+g+VHMf8ACD2/fWda/wDA0/4Vr6RoWn6HA8emweX5h3SOzFmkPqSeT3/OtCimoxWyCyQUUUVQwooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArgvFvgzVNb+Kng7xDZG3Flopn+0iRyHO9QBtGOfzFdteO6W58qWOJ2ZUV5OgywH4nngdziuf8OazqF5p6anrF5Zx20kbvHDHEQxj80iKUtu/iTBIx1PHTkA6eiqWnazpurq7aVfW94sYRmMEgcAOgdDx6qysPUEGrtABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUVl6/fyafps9xHcRQCGCWdiw3MwRd2FH8z2/HIAOY8O+D9T0v4weKvEt0bf7Bq0NuluEkJfKKAdwxxyPU13dcw/iG60Dwvc6x4mlhmkihjZrSyhIdJWQfuRljuLMQFzjqPWuljcSRq46MARmgS1HUUUUDCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigCC8sra/gEN5EssYdXAbsynII9CCM5rPu/DdlN4dvNIst2nx3NqbZJLY4aBdpClP7u3OVA4B6Vr0UAZ2i6PHpFp5a+WZCqoxjj2KFRQqKq5OAAOmfWtGiigNgooooAKKKKACiiigAooooAKKKKACiiigAooooAKpaho+n6qANStIrkBHjxIuQUbG5T6g4GQeDirtFAGRq/h+HUtNS0iZY9t1DdZkUyB3jdXXdkgnlF7/AMI+lasSNHCiO7SMqgF2xlj6nFOooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACioHv7SO+Sye6gW6kXckBkAkZfUL1I4NT0AFFFQWt/aX3mfYrqC48pyknkyB9jDqpx0PtQBPRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRQBgYoAKKKKAOD06ceMfiL4itNVjE2maA0NtBZyDMckrp5jSup4YjKhc9ME9TW0nhaOz8Z2Wrabi2tIrSeGW0jJWMu7RkOEHAPysCe/FNk8OXGm+KrvX/D/kGbUY44760uHKJMU4SQOoYqwBIPykEY6YzVCPwTLN8Qo/FU8Wn2ksdjLarDBH5h3s4YTFyq5bgjGPxOaE7W9P0/X9fIff+v6t/W5p2njLT7qXTh5F3BFqcskNpNPEEWSRAxK4zuUkIxGQM4+lQaP4lvNR8aa7pcunzx2+nNBGrlo8KWQuWPzZ5BXgZ6duawH8Ca61v4fmE2nHVdJuGnuL+R2eS7Yxuhckpkff3beRxtyAAa34PDuoWnirV76CeH7LqzW7zMWIkTy02MoXaQQwA5yCMn0FN+QizD4w06eO2uFScWN3c/ZYL0qvlSSZKjHO7BYEAkYJ78gnl/iXrVnrPwp8T/ZLOW8toLeaIXQVDGJUypK5bcdrDGQOucdDWnoPgttG0ey0V7DSbq1sZVMN5Mm6XYr7lJQpjeOBu3dRu9qzNd+H+sT/AAz1XwdodzZxxXUkjwXNxI4ZVeUylGUKe5K7gehzjsTT+vl/wf63qPxL+u//AAD0C0/48oP+ua/yrnn8dWaSa6jabqOdBP8App2R4C+WJAynf8w2nOOvHTpnobNZksYUuVRJVQB1jcsoIHYkDP5CuKn8K69IPGypHp2PEK7bYm7k/d/uBF8/7rjpnjPpSm3q15/eKmlZKXkb0Hi2znu9OgS2ulGqQvNZSOqqs4VQ2AC2VJU5G4D8Kor8QdPbw3Brf9n6gLWW/wD7PZdsW+KTz/Iyw3427+4J47VVTw3rv9peEJ3i08R6HE8dzi6cl90Xl5T91z0zzj0qhd+BtcTwedC0+TT5Aus/2is08zpuT7X9o2EBDg9Rnnp78XaPNbpf8L/5ERbcbvf/AIH+ZZUrYfGLXLm3tPMk/sC3kZIgqtIwml6k4GcDqfSp9L8R3Gu/Cg6xqFtqNubjTjO8lnJEkuGUktEd2AVHQnFW4tB1RviDe6zOlmljdaXHZYS4ZpVZXdidpQDHz469qraX4Y1rTfh63hzzLOQwaabG3PmsFkOCvmMdmV4x8o3d+e9Zy/htden3v/gGuntL+n5L/gli18W2dnb+HLJLbVLs6vaBrOaTy3d9sYbEh3DD7eScYznmnS+PdOh0/VL2SzvhDo8rRX52J/o5GCSfm+YbSG+XPHvxWdb+F9ehuPBjtHpxXQbdornF3J85MPl5T91z0zzj0qne+DPEV34W8Z6YE0xZfENxLLAxu5NsQeJE+b911GzPHrWjtzvtr+a/S5jC/Kr+X5anYya9B/aUlhZQTX1zDCs0qW+z92rZ2ZLMoy204Ht2quvi7S5bWxltGkuZL+V4be3iUeYzpnzFIJAXbtOckYxjuKpWWganpOuXur2EdpNLqVvClzby3LqqSRKVVlcRkkEHBBUdM96o6f4BfSJNJv7S4jn1Gxubu4mEpKRzfaSWkAwCVwcY4P3cHrkTpcpXt/Xb/Mq6DfW2l+OPHeoT2ctpDBFaTTRrEGc4jclgEzuJ9sk11Nl4itL/AFRdMkt57a5ltBdxx3Cr+9iJAJGCehIBBweRXLy+EvFQ1HxjeWN5Y2k+uxwC0lSeTdbtGmwg/IOozhhyCenFWofB+o2vjax1zT002yijsGtJ4IyzMcyI5beUG8kKVyQCM5yelC3Se3/Af62DZO39bf8ABHfDCKOHQdWihRY4012/VUQYCjz24AreuvEMFp4ostCktbkz3sEs8MqhPLIj27lPzZB+Ydse9UfBWialoNhqEGqi13XOo3F5GbaZpBtlkLgHci4IzjvUur6HdXPizR9csmhZtPiuIXhlcoGWUJyCFPIKDjHf2o7X7fp/mU7c0n5v8ypJ4/sIfDWoa3Lp+oLb6bePZ3Ue2MyIyMFZsB8FckdCT7VpQ+I4Jta/sl7W6gvGtDdxJIEHmxhtpxhjggleGx94e+ORm8FeI5fA/iPRj/ZZudV1KW7ik+0yBFR5A+G/dkgjGOM59q3X0bWX+INpr3k2ItodLks2T7U+/ezo+R+7xj5Mdc85x2pLW1+2v/gP+ZLvd276el/8jBi8WvpvgfTrnRU1e+FzrYsXl1OWJ5kJuyjqTuwRwyrjoMcjFeiQyNLCrvE8LEcxuQWX67SR+RrgbLwNrCeC4tMunsVvLbWzqsXlzO0cg+1GbYxKArwcZAPPNdzYrdJaj7e6vOWZjs5VQWJCg4GcDAzgZxmq0t8/wsv1uN7+X/Bf6WMu3YL481NmIAGm2pJPb95cVRtZp7j4iRTSuwhk0uRoYTwEXzUwcep6/kO1Wvs0V3411a3uAWik0y1VgGK5HmXHcc1H/YF4vjOPU0KmzS0MADXspkyXDbsEYxgdM4prf5fod1CVNc/M9bfojDTWI49SXTdTtVtb177bb6wOYp8PkgSdQ+Pk2dO3Tiuo0v8A5GPXP+ukP/ooVW/sG5k8PpoVwtu9mirGLgyN5hQHg7dvD8dd3Xn2qzpf/Ix65/10h/8ARQpSa0sPGVKc7cnn/wAP5X7dPmbFFFFScAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAVDplqdWGpbZFuhGIiyzOqsoJIDKDtbG5sZBxmrdFFABVWy022sJLiS2WTfcvvlaSZ5CxxjqxOBjsOBVqigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigD//2Q=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GMU.JPG](attachment:GMU.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMU(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(GMU, self).__init__()\n",
    "        self.Dense1 = tf.keras.layers.Dense(512, activation='tanh', kernel_regularizer=l2(0.01))\n",
    "        self.Dense2 = tf.keras.layers.Dense(512, activation='tanh', kernel_regularizer=l2(0.01))\n",
    "        self.z = tf.keras.layers.Dense(512, activation='sigmoid', kernel_regularizer=l2(0.01))\n",
    "         \n",
    "    def call(self, inputs):\n",
    "        img = self.Dense1(inputs[0])\n",
    "        txt = self.Dense2(inputs[1])\n",
    "        concat =tf.keras.layers.concatenate([inputs[0],inputs[0]])\n",
    "        z    =  self.z(concat)\n",
    "        hv= tf.keras.layers.multiply([img,z])\n",
    "        lam= tf.keras.layers.Lambda(lambda x: 1.-x)(z)\n",
    "        ht=tf.keras.layers.multiply([lam,txt])\n",
    "        h=tf.keras.layers.Add()([hv,ht])\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rob(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Rob, self).__init__()\n",
    "        self.GlobalMaxPool1D = tf.keras.layers.GlobalMaxPool1D()\n",
    "        self.GlobalAveragePooling2D = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.ResNet=tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet', input_tensor=None, input_shape=(200,200,3), pooling=False, classes=2)\n",
    "        self.bert=TFAlbertModel.from_pretrained('albert-base-v2')\n",
    "        #self.Flat=tf.keras.layers.Flatten()\n",
    "        self.block = MLPBlock()\n",
    "        self.GMU= GMU()\n",
    "        self.Attention = Attention()\n",
    "        self.ResNet.trainable=False\n",
    "        self.bert.trainable=False\n",
    "        \n",
    "    def call(self, inputs,Attention=False):\n",
    "        input_ids_c = tf.keras.layers.concatenate([inputs[0],inputs[2]])\n",
    "        mask_c =  tf.keras.layers.concatenate([inputs[1],inputs[3]])\n",
    "        text_embeddings = bert(input_ids_c, attention_mask=mask_c)[0]  #We take the first value from tuple which is the pooled_output which is the embedding hence [0]\n",
    "        text_feat = self.GlobalMaxPool1D(text_embeddings)\n",
    "        image_embeddings = self.ResNet(inputs[4])\n",
    "        #self.Flat=tf.keras.layers.Flatten() Can use GlobalAveragePooling2D or Flatten but when used Flatten caused overfitting issues\n",
    "        #https://stackoverflow.com/questions/49295311/what-is-the-difference-between-flatten-and-globalaveragepooling2d-in-keras#:~:text=Flatten%20accepts%20as%20input%20tensor,H*W*n_channels)%20.&text=GlobalAveragePooling2D%20accepts%20as%20input%204D%20tensor.\n",
    "        \n",
    "        \n",
    "        image_feat = self.GlobalAveragePooling2D(image_embeddings)\n",
    "        #x = tf.keras.layers.concatenate([text_feat, image_feat])#Combining text and image vectors(FUSION) into a single vector\n",
    "        gm= self.GMU([image_feat,text_feat])\n",
    "        if Attention: #Applying Attention mechanism to the fused vector\n",
    "            x = self.Attention(gm)\n",
    "            x = self.block(x)\n",
    "        else:\n",
    "            x = self.block(gm)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at albert-base-v2 were not used when initializing TFAlbertModel: ['predictions']\n",
      "- This IS expected if you are initializing TFAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFAlbertModel were initialized from the model checkpoint at albert-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention NO\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 250)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 250)]        0           []                               \n",
      "                                                                                                  \n",
      " input_ids2 (InputLayer)        [(None, 245)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask2 (InputLayer)   [(None, 245)]        0           []                               \n",
      "                                                                                                  \n",
      " images (InputLayer)            [(None, 200, 200, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " rob_5 (Rob)                    (None, 1)            38850023    ['input_ids[0][0]',              \n",
      "                                                                  'attention_mask[0][0]',         \n",
      "                                                                  'input_ids2[0][0]',             \n",
      "                                                                  'attention_mask2[0][0]',        \n",
      "                                                                  'images[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38,850,023\n",
      "Trainable params: 3,600,615\n",
      "Non-trainable params: 35,249,408\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_len=250\n",
    "optimizer= tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "seq_len2=245\n",
    "input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(seq_len,), name='attention_mask', dtype='int32')\n",
    "input_ids2 = tf.keras.layers.Input(shape=(seq_len2,), name='input_ids2', dtype='int32')\n",
    "mask2 = tf.keras.layers.Input(shape=(seq_len2,), name='attention_mask2', dtype='int32')\n",
    "image_inputs=tf.keras.layers.Input(shape=(200,200,3),name=\"images\")\n",
    "ins=[input_ids, mask,input_ids2, mask2, image_inputs]\n",
    "outs = Rob()(ins)\n",
    "model = tf.keras.Model(ins, outs)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model.summary()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(tweet,img,tweet_text,tweet_txt_img):\n",
    "    optimizer= tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    checkpoint_filepath = r'C:\\Users\\UWAMAHIRWE\\Desktop\\Thesis\\crypto_data\\model_wt_full_albert_res.ckpt'\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "    early_stop=tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=4, verbose=1)\n",
    "    seq_len=250\n",
    "    seq_len2=245\n",
    "    if img:\n",
    "        ins = tf.keras.layers.Input(shape=(200, 200, 3),name='images1')\n",
    "        outs = Images()(ins)\n",
    "        model = tf.keras.Model(ins, outs)\n",
    "        model.summary()\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "        history=model.fit(train,validation_data=test, epochs=10,callbacks=[model_checkpoint_callback,early_stop])\n",
    "    elif tweet_txt_img:\n",
    "        input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')\n",
    "        mask = tf.keras.layers.Input(shape=(seq_len,), name='attention_mask', dtype='int32')\n",
    "        input_ids2 = tf.keras.layers.Input(shape=(seq_len2,), name='input_ids2', dtype='int32')\n",
    "        mask2 = tf.keras.layers.Input(shape=(seq_len2,), name='attention_mask2', dtype='int32')\n",
    "        image_inputs=tf.keras.layers.Input(shape=(200,200,3),name=\"images\")\n",
    "        ins=[input_ids, mask,input_ids2, mask2, image_inputs]\n",
    "        outs = Rob()(ins)\n",
    "        model = tf.keras.Model(ins, outs)\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "        model.summary()    \n",
    "        history=model.fit(train,validation_data=test, epochs=10,callbacks=[model_checkpoint_callback,early_stop])\n",
    "    elif tweet_text:\n",
    "        input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')\n",
    "        mask = tf.keras.layers.Input(shape=(seq_len,), name='attention_mask', dtype='int32')\n",
    "        input_ids2 = tf.keras.layers.Input(shape=(seq_len2,), name='input_ids2', dtype='int32')\n",
    "        mask2 = tf.keras.layers.Input(shape=(seq_len2,), name='attention_mask2', dtype='int32')\n",
    "        ins=[input_ids, mask,input_ids2, mask2]\n",
    "        outs = Rob()(ins)\n",
    "        model = tf.keras.Model(ins, outs)\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        history=model.fit(train,validation_data=test, epochs=10,callbacks=[model_checkpoint_callback,early_stop])\n",
    "        \n",
    "    elif tweet:\n",
    "        input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')\n",
    "        mask = tf.keras.layers.Input(shape=(seq_len,), name='attention_mask', dtype='int32')\n",
    "        ins=[input_ids, mask]\n",
    "        outs = Rob()(ins)\n",
    "        model = tf.keras.Model(ins, outs)\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        history=model.fit(train,validation_data=test, epochs=10,callbacks=[model_checkpoint_callback,early_stop])\n",
    "    \n",
    "    return history,model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=run_model(tweet=False,img=False,tweet_text=False,tweet_txt_img=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=plt.plot(model[0].history['accuracy'],linestyle='-', marker='s', color='r')\n",
    "y=plt.plot(model[0].history['val_accuracy'],linestyle='-', marker='s', color='b')\n",
    "def plot(train,test):\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.xlim(1, 10)\n",
    "    plt.ylim(0.1,1 )\n",
    "    plt.grid()\n",
    "    return plt.show()\n",
    "plot(x,y)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=model[1].predict(test)\n",
    "ypred=np.where(z<=0.49,0,1)\n",
    "ytest=[k for i,j in test for k in j.numpy()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_mat(ytest,ypred):\n",
    "    cm = confusion_matrix(ytest, ypred)\n",
    "    labels = [\"Negative\",\"Positive\"]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "\n",
    "    disp.plot(cmap=plt.cm.Blues,values_format='d')\n",
    "    return plt.show()\n",
    "\n",
    "confusion_mat(ytest,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision: {}\".format(precision_score(ytest, ypred)))\n",
    "print('Recall: %.3f' % recall_score(ytest, ypred))\n",
    "print('Accuracy: %.3f' % accuracy_score(ytest, ypred))\n",
    "print('F1 Score: %.3f' % f1_score(ytest, ypred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\"Negative\",\"Positive\"]\n",
    "print(classification_report(ytest, ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred=ypred.flatten()\n",
    "test_1['Pred Target']=ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1.to_csv(\"Predicted Tweet+Text+Img Albert+ResNet.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
